{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pickle\n",
    "\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "class WhitespaceTokenizer(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, text):\n",
    "        words = text.split()\n",
    "        # All tokens 'own' a subsequent space character in this tokenizer\n",
    "        spaces = [True] * len(words)\n",
    "        return Doc(self.vocab, words=words, spaces=spaces)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)\n",
    "\n",
    "def local(text, len_aspect, position):\n",
    "    SRD = 10\n",
    "    if position - SRD < 0:\n",
    "        begin = 0\n",
    "    else:\n",
    "        begin = position - SRD\n",
    "    if position + len_aspect + SRD < len(text.split()):\n",
    "        end = position + len_aspect + SRD\n",
    "    else:\n",
    "        end = len(text.split())\n",
    "    local_context = [i for i in text.split()][begin : end]\n",
    "    return ' '.join(local_context)\n",
    "\n",
    "def dependency_adj_matrix(text):\n",
    "    tokens = nlp(text)\n",
    "    words = text.split()\n",
    "    matrix = np.zeros((len(words), len(words))).astype('float32')\n",
    "    assert len(words) == len(list(tokens))\n",
    "\n",
    "    for token in tokens:\n",
    "        matrix[token.i][token.i] = 1\n",
    "        for child in token.children:\n",
    "            matrix[token.i][child.i] = 1\n",
    "            matrix[child.i][token.i] = 1\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def dependency_adj_matrix2(text, lcf, local_position, matrix):\n",
    "    tokens = nlp(text)\n",
    "    text_list = text.split()\n",
    "    seq_len = len(text_list)\n",
    "    sub_matrix = np.zeros((seq_len, seq_len)).astype('float32')\n",
    "    lcf_list = lcf.split()\n",
    "\n",
    "    #处理lcf部分\n",
    "    for token in tokens:\n",
    "        #如果当前节点是lcf,只有在里面的时候才会更改值\n",
    "        if str(token) in lcf_list:\n",
    "            #如果当前节点是aspect\n",
    "            for j in range(seq_len):\n",
    "                #只有当对方也是lcf才会改变\n",
    "                if text_list[j] in lcf_list:\n",
    "                    sub_matrix[token.i][j] = 1\n",
    "                else:\n",
    "                    sub_matrix[token.i][j] = 1 / 2 * (1 / (abs(j - local_position) + 1) + 1 / (abs(token.i - local_position) + 1))\n",
    "        else:\n",
    "            for j in range(seq_len):\n",
    "                sub_matrix[token.i][j] = 1 / 2 * (1 / (abs(j - local_position) + 1) + 1 / (abs(token.i - local_position) + 1))\n",
    "    return matrix * sub_matrix\n",
    "\n",
    "def process(filename):\n",
    "    fin = open(filename, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    lines = fin.readlines()\n",
    "    fin.close()\n",
    "    idx2graph = {}\n",
    "    idx2tree = {}\n",
    "    fout = open(filename+'.graph', 'wb')\n",
    "    fout2 = open(filename+'.tree', 'wb')\n",
    "    for i in range(0, len(lines), 3):\n",
    "        text_left, _, text_right = [s.lower().strip() for s in lines[i].partition(\"$T$\")]\n",
    "        aspect = lines[i + 1].lower().strip()\n",
    "        text = text_left + ' ' + aspect + ' ' + text_right\n",
    "        local_position = len(text_left.split()) + len(aspect.split()) / 2\n",
    "        lcf = local(text, len(aspect.split()), len(text_left.split()))\n",
    "        adj_matrix = dependency_adj_matrix(text)\n",
    "        matrix = dependency_adj_matrix2(text, lcf, local_position, adj_matrix)\n",
    "        idx2graph[i] = matrix\n",
    "        idx2tree[i] = adj_matrix\n",
    "    pickle.dump(idx2graph, fout)        \n",
    "    fout.close()\n",
    "    pickle.dump(idx2tree, fout2)        \n",
    "    fout2.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    process('./datasets/semeval14/restaurant_train.raw')\n",
    "    process('./datasets/semeval14/restaurant_test.raw')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
